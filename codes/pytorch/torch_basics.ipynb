{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch basics for DL newbies\n",
    "\n",
    "[![Open In Colab]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4],\n",
       "       [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_array = np.arange(10).reshape(2,5)\n",
    "n_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_array.ndim\n",
    "n_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t_array = torch.FloatTensor(n_array)\n",
    "t_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_array.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "2\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "print(t_array.shape)\n",
    "print(t_array.ndim)\n",
    "print(t_array.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_array[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2.],\n",
       "        [5., 6., 7.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_array[:2, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = np.arange(10).reshape(2,5)\n",
    "n2 = np.arange(10).reshape(5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of t1:  torch.Size([2, 5])\n",
      "Shape of t2:  torch.Size([5, 2])\n",
      "tensor([[ 60.,  70.],\n",
      "        [160., 195.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.FloatTensor(n1)\n",
    "t2 = torch.FloatTensor(n2)\n",
    "print('Shape of t1: ', t1.shape)\n",
    "print('Shape of t2: ', t2.shape)\n",
    "print(t1.matmul(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 60,  70],\n",
       "       [160, 195]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.dot(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0f434319db61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "t1 * t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [4., 9.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = np.arange(4).reshape(2,2)\n",
    "n2 = np.arange(4).reshape(2,2)\n",
    "t1 = torch.FloatTensor(n1)\n",
    "t2 = torch.FloatTensor(n2)\n",
    "\n",
    "t1 * t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [4., 9.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.mul(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  5.],\n",
       "        [10., 15.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = np.arange(10)\n",
    "t1 = torch.FloatTensor(n1)\n",
    "t1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = np.arange(10).reshape(5,2)\n",
    "t1 = torch.FloatTensor(n1)\n",
    "t1.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 2.5000, 4.5000, 6.5000, 8.5000])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.],\n",
       "        [4., 5.],\n",
       "        [6., 7.],\n",
       "        [8., 9.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = np.arange(10)\n",
    "t1 = torch.FloatTensor(n1)\n",
    "t1.view(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.view(-1, 10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.view(-1, 10).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.view(-1, 10).squeeze().unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor operations for ML/DL formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3458, 0.4224, 0.2318])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.FloatTensor([0.5, 0.7, 0.1])\n",
    "h_tensor = F.softmax(tensor, dim=0)\n",
    "h_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randint(5, (10,5))\n",
    "y_label = y.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2, 0, 0, 1, 1, 1, 4, 2])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.one_hot(y_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch autogard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = w^2 \\\\ \n",
    "z = 2*y + 5 \\\\\n",
    "z = 2*w^2 + 5 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "y = w**2\n",
    "z = 2*y + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q = 3a^3 - b^2  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-dd54d3768976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexternal_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexternal_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    121\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    122\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial Q}{\\partial a} = 9a^2 $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial Q}{\\partial b} = -2b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 72., 162.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-24., -16.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AutoGrad for Linear Regression\n",
    "https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y=2x+1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(253.5460, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 0, loss 253.54598999023438\n",
      "tensor(20.8421, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 1, loss 20.84214210510254\n",
      "tensor(1.8594, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 2, loss 1.8594218492507935\n",
      "tensor(0.3093, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 3, loss 0.3092811107635498\n",
      "tensor(0.1811, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 4, loss 0.18108133971691132\n",
      "tensor(0.1689, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 5, loss 0.16888414323329926\n",
      "tensor(0.1662, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 6, loss 0.16616836190223694\n",
      "tensor(0.1642, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 7, loss 0.16424500942230225\n",
      "tensor(0.1624, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 8, loss 0.162405326962471\n",
      "tensor(0.1606, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 9, loss 0.1605914682149887\n",
      "tensor(0.1588, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 10, loss 0.1587979942560196\n",
      "tensor(0.1570, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 11, loss 0.15702475607395172\n",
      "tensor(0.1553, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 12, loss 0.1552712321281433\n",
      "tensor(0.1535, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 13, loss 0.15353739261627197\n",
      "tensor(0.1518, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 14, loss 0.15182292461395264\n",
      "tensor(0.1501, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 15, loss 0.15012750029563904\n",
      "tensor(0.1485, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 16, loss 0.14845092594623566\n",
      "tensor(0.1468, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 17, loss 0.14679335057735443\n",
      "tensor(0.1452, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 18, loss 0.14515411853790283\n",
      "tensor(0.1435, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 19, loss 0.14353306591510773\n",
      "tensor(0.1419, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 20, loss 0.14193038642406464\n",
      "tensor(0.1403, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 21, loss 0.14034545421600342\n",
      "tensor(0.1388, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 22, loss 0.13877823948860168\n",
      "tensor(0.1372, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 23, loss 0.1372285783290863\n",
      "tensor(0.1357, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 24, loss 0.13569612801074982\n",
      "tensor(0.1342, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 25, loss 0.13418079912662506\n",
      "tensor(0.1327, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 26, loss 0.13268247246742249\n",
      "tensor(0.1312, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 27, loss 0.1312008500099182\n",
      "tensor(0.1297, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 28, loss 0.12973566353321075\n",
      "tensor(0.1283, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 29, loss 0.1282869130373001\n",
      "tensor(0.1269, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 30, loss 0.12685447931289673\n",
      "tensor(0.1254, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 31, loss 0.12543781101703644\n",
      "tensor(0.1240, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 32, loss 0.12403710931539536\n",
      "tensor(0.1227, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 33, loss 0.12265194952487946\n",
      "tensor(0.1213, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 34, loss 0.12128230929374695\n",
      "tensor(0.1199, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 35, loss 0.11992799490690231\n",
      "tensor(0.1186, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 36, loss 0.11858877539634705\n",
      "tensor(0.1173, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 37, loss 0.1172645315527916\n",
      "tensor(0.1160, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 38, loss 0.11595506221055984\n",
      "tensor(0.1147, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 39, loss 0.11466018855571747\n",
      "tensor(0.1134, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 40, loss 0.11337978392839432\n",
      "tensor(0.1121, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 41, loss 0.11211369931697845\n",
      "tensor(0.1109, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 42, loss 0.11086181551218033\n",
      "tensor(0.1096, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 43, loss 0.10962378978729248\n",
      "tensor(0.1084, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 44, loss 0.10839961469173431\n",
      "tensor(0.1072, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 45, loss 0.10718919336795807\n",
      "tensor(0.1060, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 46, loss 0.10599226504564285\n",
      "tensor(0.1048, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 47, loss 0.1048085168004036\n",
      "tensor(0.1036, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 48, loss 0.10363820940256119\n",
      "tensor(0.1025, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 49, loss 0.10248090326786041\n",
      "tensor(0.1013, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 50, loss 0.10133654624223709\n",
      "tensor(0.1002, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 51, loss 0.10020491480827332\n",
      "tensor(0.0991, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 52, loss 0.09908589720726013\n",
      "tensor(0.0980, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 53, loss 0.09797954559326172\n",
      "tensor(0.0969, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 54, loss 0.09688524156808853\n",
      "tensor(0.0958, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 55, loss 0.09580349177122116\n",
      "tensor(0.0947, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 56, loss 0.09473363310098648\n",
      "tensor(0.0937, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 57, loss 0.09367572516202927\n",
      "tensor(0.0926, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 58, loss 0.09262967109680176\n",
      "tensor(0.0916, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 59, loss 0.09159530699253082\n",
      "tensor(0.0906, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 60, loss 0.09057246148586273\n",
      "tensor(0.0896, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 61, loss 0.08956105262041092\n",
      "tensor(0.0886, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 62, loss 0.08856097608804703\n",
      "tensor(0.0876, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 63, loss 0.08757199347019196\n",
      "tensor(0.0866, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 64, loss 0.0865940973162651\n",
      "tensor(0.0856, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 65, loss 0.08562712371349335\n",
      "tensor(0.0847, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 66, loss 0.08467087894678116\n",
      "tensor(0.0837, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 67, loss 0.0837254673242569\n",
      "tensor(0.0828, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 68, loss 0.08279047906398773\n",
      "tensor(0.0819, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 69, loss 0.08186594396829605\n",
      "tensor(0.0810, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 70, loss 0.08095184713602066\n",
      "tensor(0.0800, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 71, loss 0.08004781603813171\n",
      "tensor(0.0792, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 72, loss 0.07915392518043518\n",
      "tensor(0.0783, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 73, loss 0.07827004045248032\n",
      "tensor(0.0774, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 74, loss 0.07739603519439697\n",
      "tensor(0.0765, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 75, loss 0.07653174549341202\n",
      "tensor(0.0757, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 76, loss 0.0756770670413971\n",
      "tensor(0.0748, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 77, loss 0.07483205944299698\n",
      "tensor(0.0740, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 78, loss 0.07399637252092361\n",
      "tensor(0.0732, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 79, loss 0.07317008823156357\n",
      "tensor(0.0724, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 80, loss 0.0723530650138855\n",
      "tensor(0.0715, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 81, loss 0.0715450644493103\n",
      "tensor(0.0707, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 82, loss 0.07074613869190216\n",
      "tensor(0.0700, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 83, loss 0.06995614618062973\n",
      "tensor(0.0692, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 84, loss 0.06917494535446167\n",
      "tensor(0.0684, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 85, loss 0.0684024766087532\n",
      "tensor(0.0676, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 86, loss 0.06763867288827896\n",
      "tensor(0.0669, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 87, loss 0.0668833777308464\n",
      "tensor(0.0661, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 88, loss 0.0661364197731018\n",
      "tensor(0.0654, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 89, loss 0.06539791077375412\n",
      "tensor(0.0647, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 90, loss 0.06466758996248245\n",
      "tensor(0.0639, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 91, loss 0.06394554674625397\n",
      "tensor(0.0632, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 92, loss 0.06323138624429703\n",
      "tensor(0.0625, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 93, loss 0.06252533197402954\n",
      "tensor(0.0618, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 94, loss 0.06182710826396942\n",
      "tensor(0.0611, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 95, loss 0.06113667041063309\n",
      "tensor(0.0605, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 96, loss 0.06045397371053696\n",
      "tensor(0.0598, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 97, loss 0.05977889150381088\n",
      "tensor(0.0591, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 98, loss 0.05911136791110039\n",
      "tensor(0.0585, device='cuda:0', grad_fn=<MseLossBackward>)\n",
      "epoch 99, loss 0.058451294898986816\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5502644]\n",
      " [ 2.6150303]\n",
      " [ 4.679796 ]\n",
      " [ 6.744562 ]\n",
      " [ 8.809328 ]\n",
      " [10.874094 ]\n",
      " [12.93886  ]\n",
      " [15.003626 ]\n",
      " [17.068392 ]\n",
      " [19.133158 ]\n",
      " [21.197924 ]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 CONDA (NGC/PyTorch 20.06) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
